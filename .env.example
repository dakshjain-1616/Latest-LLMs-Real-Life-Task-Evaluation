# LLM Benchmark Configuration Template
# Copy this file to .env and fill in your actual values.
# DO NOT commit your .env file to version control.

# --- LLM API Keys ---
# Provide at least one key below. The tool will use native providers if keys are present,
# otherwise it defaults to OpenRouter for supported models.

OPENAI_API_KEY=
OPENROUTER_API_KEY=
ANTHROPIC_API_KEY=
GOOGLE_API_KEY=
ZHIPU_API_KEY=

# --- Judge Configuration ---
# Used for LLM-as-judge subjective evaluations
JUDGE_MODEL_PROVIDER=openai
JUDGE_MODEL_NAME=gpt-4o
OPENAI_ORG_ID=

# --- Benchmark Execution ---
MAX_CONCURRENT_REQUESTS=5
REQUEST_TIMEOUT=120
RETRY_MAX_ATTEMPTS=3
RETRY_BACKOFF_FACTOR=2

# --- Storage & Logging ---
RESULTS_DIR=./results
DB_PATH=./results/benchmark.db
LOG_LEVEL=INFO

# --- Observability (Optional) ---
MLFLOW_TRACKING_URI=
WANDB_API_KEY=
WANDB_PROJECT=llm-benchmark-suite
