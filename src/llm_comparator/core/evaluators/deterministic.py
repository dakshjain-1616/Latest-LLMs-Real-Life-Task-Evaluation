"""
Deterministic Evaluators
Scoring functions for objective, reproducible evaluation.
"""

import json
import re
import sqlite3
import subprocess
import tempfile
from typing import Dict, Any, Tuple
from pathlib import Path
import logging

try:
    import jsonschema
except ImportError:
    jsonschema = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def exact_match_evaluator(output: str, ground_truth: str, **kwargs) -> Tuple[float, str]:
    """
    Exact character-level match evaluation.
    
    Args:
        output: Model output string
        ground_truth: Expected output string
        
    Returns:
        Tuple of (score, justification)
    """
    output_clean = output.strip()
    ground_truth_clean = ground_truth.strip()
    
    if output_clean == ground_truth_clean:
        return 1.0, "Exact match"
    else:
        return 0.0, f"Mismatch: expected '{ground_truth_clean}', got '{output_clean}'"


def json_schema_evaluator(output: str, schema: Dict[str, Any], **kwargs) -> Tuple[float, str]:
    """
    JSON schema compliance validation.
    
    Args:
        output: Model output (should be JSON string)
        schema: JSON schema to validate against
        
    Returns:
        Tuple of (score, justification)
    """
    if jsonschema is None:
        return 0.0, "jsonschema library not installed"
    
    try:
        # Parse JSON
        parsed = json.loads(output)
        
        # Validate against schema
        jsonschema.validate(instance=parsed, schema=schema)
        return 1.0, "Valid JSON matching schema"
    
    except json.JSONDecodeError as e:
        return 0.0, f"Invalid JSON: {str(e)}"
    
    except jsonschema.ValidationError as e:
        return 0.0, f"Schema validation failed: {e.message}"
    
    except Exception as e:
        return 0.0, f"Evaluation error: {str(e)}"


def regex_evaluator(output: str, pattern: str, **kwargs) -> Tuple[float, str]:
    """
    Regex pattern matching evaluation.
    
    Args:
        output: Model output string
        pattern: Regex pattern to match
        
    Returns:
        Tuple of (score, justification)
    """
    try:
        compiled_pattern = re.compile(pattern, re.DOTALL)
        match = compiled_pattern.search(output)
        
        if match:
            return 1.0, f"Pattern matched: {match.group(0)[:100]}"
        else:
            return 0.0, f"Pattern '{pattern}' not found in output"
    
    except re.error as e:
        return 0.0, f"Invalid regex pattern: {str(e)}"


def sql_execution_evaluator(
    output: str,
    test_db_path: str,
    expected_result: Any,
    **kwargs
) -> Tuple[float, str]:
    """
    SQL query execution validation.
    
    Args:
        output: SQL query string from model
        test_db_path: Path to test SQLite database
        expected_result: Expected query result
        
    Returns:
        Tuple of (score, justification)
    """
    try:
        # Extract SQL query from output (handle markdown code blocks)
        sql_query = output.strip()
        
        # Remove markdown code fences if present
        if sql_query.startswith("```"):
            lines = sql_query.split('\n')
            sql_query = '\n'.join(lines[1:-1]) if len(lines) > 2 else sql_query
            sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
        
        # Connect to test database
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()
        
        # Execute query
        cursor.execute(sql_query)
        result = cursor.fetchall()
        
        conn.close()
        
        # Compare result with expected
        if result == expected_result:
            return 1.0, f"Query executed successfully with correct result"
        else:
            return 0.5, f"Query executed but result mismatch: expected {expected_result}, got {result}"
    
    except sqlite3.Error as e:
        return 0.0, f"SQL execution error: {str(e)}"
    
    except Exception as e:
        return 0.0, f"Evaluation error: {str(e)}"


def unit_test_evaluator(
    output: str,
    test_file_path: str,
    language: str = "python",
    **kwargs
) -> Tuple[float, str]:
    """
    Unit test execution validation.
    
    Args:
        output: Code generated by model
        test_file_path: Path to unit test file
        language: Programming language (default: python)
        
    Returns:
        Tuple of (score, justification)
    """
    try:
        # Create temporary file for generated code
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(output)
            temp_code_path = f.name
        
        if language == "python":
            # Run pytest on the test file with the generated code
            result = subprocess.run(
                ['python', '-m', 'pytest', test_file_path, '-v', '--tb=short'],
                capture_output=True,
                text=True,
                timeout=30,
                env={'PYTHONPATH': str(Path(temp_code_path).parent)}
            )
            
            # Parse test results
            if result.returncode == 0:
                return 1.0, "All unit tests passed"
            else:
                # Count passed/failed tests
                output_lines = result.stdout + result.stderr
                if "passed" in output_lines:
                    return 0.5, f"Some tests failed: {output_lines[-500:]}"
                else:
                    return 0.0, f"All tests failed: {output_lines[-500:]}"
        
        else:
            return 0.0, f"Unsupported language: {language}"
    
    except subprocess.TimeoutExpired:
        return 0.0, "Test execution timeout"
    
    except Exception as e:
        return 0.0, f"Test execution error: {str(e)}"
    
    finally:
        # Clean up temporary file
        try:
            Path(temp_code_path).unlink()
        except:
            pass


def code_execution_evaluator(
    output: str,
    test_cases: list,
    language: str = "python",
    **kwargs
) -> Tuple[float, str]:
    """
    Code execution with test cases.
    
    Args:
        output: Code generated by model
        test_cases: List of (input, expected_output) tuples
        language: Programming language
        
    Returns:
        Tuple of (score, justification)
    """
    if language != "python":
        return 0.0, f"Unsupported language: {language}"
    
    try:
        # Extract code from markdown if present
        code = output.strip()
        if code.startswith("```"):
            lines = code.split('\n')
            code = '\n'.join(lines[1:-1]) if len(lines) > 2 else code
            code = code.replace("```python", "").replace("```", "").strip()
        
        # Create a namespace for execution
        namespace = {}
        exec(code, namespace)
        
        # Run test cases
        passed = 0
        total = len(test_cases)
        errors = []
        
        for test_input, expected_output in test_cases:
            try:
                # Find the main function to test
                func_name = None
                for name, obj in namespace.items():
                    if callable(obj) and not name.startswith('_'):
                        func_name = name
                        break
                
                if func_name is None:
                    return 0.0, "No callable function found in code"
                
                # Execute function with test input
                result = namespace[func_name](*test_input) if isinstance(test_input, (list, tuple)) else namespace[func_name](test_input)
                
                if result == expected_output:
                    passed += 1
                else:
                    errors.append(f"Input {test_input}: expected {expected_output}, got {result}")
            
            except Exception as e:
                errors.append(f"Input {test_input}: {str(e)}")
        
        score = passed / total if total > 0 else 0.0
        justification = f"Passed {passed}/{total} test cases"
        if errors:
            justification += f". Errors: {'; '.join(errors[:3])}"
        
        return score, justification
    
    except Exception as e:
        return 0.0, f"Code execution error: {str(e)}"
